{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-21 00:43:57.462155\n"
     ]
    }
   ],
   "source": [
    "# from fiam\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# turn off pandas Setting with Copy Warning\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "# set working directory\n",
    "work_dir = \"C:/Users/akobe/OneDrive/Asset-Management-FIAM/McGill-FIAM Asset Management Hackathon/data/\"\n",
    "\n",
    "# read sample data\n",
    "file_path = os.path.join(\n",
    "    work_dir, \"hackathon_sample_v2.csv\"\n",
    ")  # replace with the correct file name\n",
    "raw = pd.read_csv(\n",
    "    file_path, parse_dates=[\"date\"], low_memory=False\n",
    ")  # the date is the first day of the return month (t+1)\n",
    "\n",
    "# read list of predictors for stocks\n",
    "file_path = os.path.join(\n",
    "    work_dir, \"factor_char_list.csv\"\n",
    ")  # replace with the correct file name\n",
    "stock_vars = list(pd.read_csv(file_path)[\"variable\"].values)\n",
    "\n",
    "# define the left hand side variable\n",
    "ret_var = \"stock_exret\" #possibly change?\n",
    "new_set = raw[\n",
    "    raw[ret_var].notna()\n",
    "].copy()  # create a copy of the data and make sure the left hand side is not missing\n",
    "\n",
    "# transform each variable in each month to the same scale\n",
    "monthly = new_set.groupby(\"date\")\n",
    "data = pd.DataFrame()\n",
    "for date, monthly_raw in monthly:\n",
    "    group = monthly_raw.copy()\n",
    "    # rank transform each variable to [-1, 1]\n",
    "    for var in stock_vars:\n",
    "        var_median = group[var].median(skipna=True)\n",
    "        group[var] = group[var].fillna(\n",
    "            var_median\n",
    "        )  # fill missing values with the cross-sectional median of each month\n",
    "\n",
    "        group[var] = group[var].rank(method=\"dense\") - 1\n",
    "        group_max = group[var].max()\n",
    "        if group_max > 0:\n",
    "            group[var] = (group[var] / group_max) * 2 - 1\n",
    "        else:\n",
    "            group[var] = 0  # in case of all missing values\n",
    "            print(\"Warning:\", date, var, \"set to zero.\")\n",
    "\n",
    "    # add the adjusted values\n",
    "    data = data._append(\n",
    "        group, ignore_index=True\n",
    "    )  # append may not work with certain versions of pandas, use concat instead if needed\n",
    "\n",
    "# initialize the starting date, counter, and output data\n",
    "starting = pd.to_datetime(\"20000101\", format=\"%Y%m%d\")\n",
    "counter = 0\n",
    "pred_out = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X, y, n_features=50):\n",
    "    \"\"\"Select top features based on F-statistic\"\"\"\n",
    "    selector = SelectKBest(f_regression, k=n_features)\n",
    "    selector.fit(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from fiam - do not run takes long to execute\n",
    "\"\"\"\n",
    "#  estimation with expanding window\n",
    "while (starting + pd.DateOffset(years=11 + counter)) <= pd.to_datetime(\n",
    "    \"20240101\", format=\"%Y%m%d\"\n",
    "):\n",
    "    cutoff = [\n",
    "        starting,\n",
    "        starting\n",
    "        + pd.DateOffset(\n",
    "            years=8 + counter\n",
    "        ),  # use 8 years and expanding as the training set\n",
    "        starting\n",
    "        + pd.DateOffset(\n",
    "            years=10 + counter\n",
    "        ),  # use the next 2 years as the validation set\n",
    "        starting + pd.DateOffset(years=11 + counter),\n",
    "    ]  # use the next year as the out-of-sample testing set\n",
    "\n",
    "    # cut the sample into training, validation, and testing sets\n",
    "    train = data[(data[\"date\"] >= cutoff[0]) & (data[\"date\"] < cutoff[1])]\n",
    "    validate = data[(data[\"date\"] >= cutoff[1]) & (data[\"date\"] < cutoff[2])]\n",
    "    test = data[(data[\"date\"] >= cutoff[2]) & (data[\"date\"] < cutoff[3])]\n",
    "\n",
    "    selected_features = select_features(train[stock_vars], train[ret_var])\n",
    "\n",
    "    # Optional: if your data has additional binary or categorical variables,\n",
    "    # you can further standardize them here\n",
    "    scaler = StandardScaler().fit(train[stock_vars])\n",
    "    train[stock_vars] = scaler.transform(train[stock_vars])\n",
    "    validate[stock_vars] = scaler.transform(validate[stock_vars])\n",
    "    test[stock_vars] = scaler.transform(test[stock_vars])\n",
    "\n",
    "    # get Xs and Ys\n",
    "    X_train = train[stock_vars].values\n",
    "    Y_train = train[ret_var].values\n",
    "    X_val = validate[stock_vars].values\n",
    "    Y_val = validate[ret_var].values\n",
    "    X_test = test[stock_vars].values\n",
    "    Y_test = test[ret_var].values\n",
    "\n",
    "    # de-mean Y (because the regressions are fitted without an intercept)\n",
    "    # if you want to include an intercept (or bias in neural networks, etc), you can skip this step\n",
    "    Y_mean = np.mean(Y_train)\n",
    "    Y_train_dm = Y_train - Y_mean\n",
    "\n",
    "    # prepare output data\n",
    "    reg_pred = test[\n",
    "        [\"year\", \"month\", \"date\", \"permno\", ret_var]\n",
    "    ]  # minimum identifications for each stock\n",
    "\n",
    "    # Linear Regression\n",
    "    # no validation is needed for OLS\n",
    "    reg = LinearRegression(fit_intercept=False)\n",
    "    reg.fit(X_train, Y_train_dm)\n",
    "    x_pred = reg.predict(X_test) + Y_mean\n",
    "    reg_pred[\"ols\"] = x_pred\n",
    "\n",
    "    # Lasso\n",
    "    lambdas = np.arange(\n",
    "        -4, 4.1, 0.1\n",
    "    )  # search for the best lambda in the range of 10^-4 to 10^4, range can be adjusted\n",
    "    val_mse = np.zeros(len(lambdas))\n",
    "    for ind, i in enumerate(lambdas):\n",
    "        reg = Lasso(alpha=(10**i), max_iter=1000000, fit_intercept=False)\n",
    "        reg.fit(X_train, Y_train_dm)\n",
    "        val_mse[ind] = mean_squared_error(Y_val, reg.predict(X_val) + Y_mean)\n",
    "\n",
    "    # select the best lambda based on the validation set\n",
    "    best_lambda = lambdas[np.argmin(val_mse)]\n",
    "    reg = Lasso(alpha=(10**best_lambda), max_iter=1000000, fit_intercept=False)\n",
    "    reg.fit(X_train, Y_train_dm)\n",
    "    x_pred = reg.predict(X_test) + Y_mean  # predict the out-of-sample testing set\n",
    "    reg_pred[\"lasso\"] = x_pred\n",
    "\n",
    "    # Ridge\n",
    "    # same format as above\n",
    "    lambdas = np.arange(-1, 8.1, 0.1)\n",
    "    val_mse = np.zeros(len(lambdas))\n",
    "    for ind, i in enumerate(lambdas):\n",
    "        reg = Ridge(alpha=((10**i) * 0.5), fit_intercept=False)\n",
    "        reg.fit(X_train, Y_train_dm)\n",
    "        val_mse[ind] = mean_squared_error(Y_val, reg.predict(X_val) + Y_mean)\n",
    "\n",
    "    best_lambda = lambdas[np.argmin(val_mse)]\n",
    "    reg = Ridge(alpha=((10**best_lambda) * 0.5), fit_intercept=False)\n",
    "    reg.fit(X_train, Y_train_dm)\n",
    "    x_pred = reg.predict(X_test) + Y_mean\n",
    "    reg_pred[\"ridge\"] = x_pred\n",
    "\n",
    "    # Elastic Net\n",
    "    # same format as above\n",
    "    lambdas = np.arange(-4, 4.1, 0.1)\n",
    "    val_mse = np.zeros(len(lambdas))\n",
    "    for ind, i in enumerate(lambdas):\n",
    "        reg = ElasticNet(alpha=(10**i), max_iter=1000000, fit_intercept=False)\n",
    "        reg.fit(X_train, Y_train_dm)\n",
    "        val_mse[ind] = mean_squared_error(Y_val, reg.predict(X_val) + Y_mean)\n",
    "\n",
    "    best_lambda = lambdas[np.argmin(val_mse)]\n",
    "    reg = ElasticNet(alpha=(10**best_lambda), max_iter=1000000, fit_intercept=False)\n",
    "    reg.fit(X_train, Y_train_dm)\n",
    "    x_pred = reg.predict(X_test) + Y_mean\n",
    "    reg_pred[\"en\"] = x_pred\n",
    "\n",
    "    # add to the output data\n",
    "    pred_out = pred_out._append(reg_pred, ignore_index=True)\n",
    "\n",
    "    # go to the next year\n",
    "    counter += 1\n",
    "\n",
    "# output the predicted value to csv\n",
    "out_path = os.path.join(work_dir, \"output.csv\")\n",
    "print(out_path)\n",
    "pred_out.to_csv(out_path, index=False)\n",
    "\n",
    "# print the OOS R2\n",
    "yreal = pred_out[ret_var].values\n",
    "for model_name in [\"ols\", \"lasso\", \"ridge\", \"en\"]:\n",
    "    ypred = pred_out[model_name].values\n",
    "    r2 = 1 - np.sum(np.square((yreal - ypred))) / np.sum(np.square(yreal))\n",
    "    print(model_name, r2)\n",
    "\n",
    "# for timing purpose\n",
    "print(datetime.datetime.now())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block provides initial tests for predicting the stock excess return - minimizing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ols - Validation MSE: 0.015196703399922797, Test MSE: 0.0070769780541416576\n",
      "---\n",
      "lasso - Validation MSE: 0.014119349956661777, Test MSE: 0.0064109213676483605\n",
      "---\n",
      "ridge - Validation MSE: 0.015196103962458237, Test MSE: 0.007076800698866414\n",
      "---\n",
      "en - Validation MSE: 0.014119349956661777, Test MSE: 0.0064109213676483605\n",
      "---\n",
      "ols - Validation MSE: 0.005725876461242681, Test MSE: 0.014633755384064666\n",
      "---\n",
      "lasso - Validation MSE: 0.00547020960108268, Test MSE: 0.01435129579188557\n",
      "---\n",
      "ridge - Validation MSE: 0.005725836485825353, Test MSE: 0.014633691176228791\n",
      "---\n",
      "en - Validation MSE: 0.00547020960108268, Test MSE: 0.01435129579188557\n",
      "---\n",
      "ols - Validation MSE: 0.01009783362922321, Test MSE: 0.0068532274721569055\n",
      "---\n",
      "lasso - Validation MSE: 0.009911206346441886, Test MSE: 0.00667082672997179\n",
      "---\n",
      "ridge - Validation MSE: 0.010097808296425901, Test MSE: 0.006853205026655427\n",
      "---\n",
      "en - Validation MSE: 0.009911206346441886, Test MSE: 0.00667082672997179\n",
      "---\n",
      "ols - Validation MSE: 0.0068048435258779546, Test MSE: 0.009636594185107957\n",
      "---\n",
      "lasso - Validation MSE: 0.006689138649950775, Test MSE: 0.009456570295705291\n",
      "---\n",
      "ridge - Validation MSE: 0.0068048321235268236, Test MSE: 0.009636578250568649\n",
      "---\n",
      "en - Validation MSE: 0.006689138649950775, Test MSE: 0.009456570295705291\n",
      "---\n",
      "ols - Validation MSE: 0.009565905753150366, Test MSE: 0.013042252563116221\n",
      "---\n",
      "lasso - Validation MSE: 0.009449561973720103, Test MSE: 0.013034856640801173\n",
      "---\n",
      "ridge - Validation MSE: 0.009565893606320847, Test MSE: 0.013042256256019949\n",
      "---\n",
      "en - Validation MSE: 0.009449561973720103, Test MSE: 0.013034856640801173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# feature scaling function\n",
    "def scale_features(train, validate, test, stock_vars):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train[stock_vars])\n",
    "    validate_scaled = scaler.transform(validate[stock_vars])\n",
    "    test_scaled = scaler.transform(test[stock_vars])\n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "# 3. model training with parallel processing\n",
    "def train_model(model, X_train, Y_train, X_val, Y_val, Y_mean):\n",
    "    model.fit(X_train, Y_train)\n",
    "    val_mse = mean_squared_error(Y_val, model.predict(X_val) + Y_mean)\n",
    "    return model, val_mse\n",
    "\n",
    "def parallel_model_training(X_train, Y_train, X_val, Y_val, Y_mean):\n",
    "    models = {\n",
    "        'ols': LinearRegression(fit_intercept=False),\n",
    "        'lasso': Lasso(max_iter=10000, fit_intercept=False),\n",
    "        'ridge': Ridge(fit_intercept=False),\n",
    "        'en': ElasticNet(max_iter=10000, fit_intercept=False)\n",
    "    }\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(train_model)(model, X_train, Y_train, X_val, Y_val, Y_mean)\n",
    "        for model in models.values()\n",
    "    )\n",
    "    \n",
    "    return dict(zip(models.keys(), results))\n",
    "\n",
    "# ltsm - in progress\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add extra dimension if input is unbatched\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.squeeze(0)  # Remove extra dimension for unbatched input\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def train_lstm(X_train, Y_train, X_val, Y_val, epochs=100, batch_size=32):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = StockLSTM(input_dim, hidden_dim=50, num_layers=2, output_dim=1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    train_dataset = StockDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(torch.FloatTensor(X_val)).squeeze().numpy()\n",
    "    \n",
    "    return model, mean_squared_error(Y_val, val_predictions)\n",
    "\n",
    "# Time series split default 5\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train_val = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "        \n",
    "    # Further split train_val into train and validate\n",
    "    train_size = int(0.8 * len(train_val))\n",
    "    train = train_val[:train_size]\n",
    "    validate = train_val[train_size:]\n",
    "        \n",
    "    # Scale features\n",
    "    X_train, X_val, X_test = scale_features(train, validate, test, stock_vars)\n",
    "    Y_train, Y_val, Y_test = train[ret_var].values, validate[ret_var].values, test[ret_var].values\n",
    "        \n",
    "        # De-mean Y\n",
    "    Y_mean = np.mean(Y_train)\n",
    "    Y_train_dm = Y_train - Y_mean\n",
    "        \n",
    "    # Train models in parallel\n",
    "    model_results = parallel_model_training(X_train, Y_train_dm, X_val, Y_val, Y_mean)\n",
    "        \n",
    "    # Train LSTM\n",
    "        # lstm_model, lstm_val_mse = train_lstm(X_train, Y_train, X_val, Y_val)\n",
    "        \n",
    "        # Make predictions and evaluate\n",
    "    for model_name, (model, val_mse) in model_results.items():\n",
    "        test_pred = model.predict(X_test) + Y_mean\n",
    "        test_mse = mean_squared_error(Y_test, test_pred)\n",
    "        print(\"---\")\n",
    "        print(f\"{model_name} - Validation MSE: {val_mse}, Test MSE: {test_mse}\")\n",
    "        \n",
    "        # LSTM prediction and evaluation\n",
    "         # lstm_model.eval()\n",
    "        #  with torch.no_grad():\n",
    "         #     lstm_test_pred = lstm_model(torch.FloatTensor(X_test)).squeeze().numpy()\n",
    "      #    lstm_test_mse = mean_squared_error(Y_test, lstm_test_pred)\n",
    "       #   print(f\"LSTM - Validation MSE: {lstm_val_mse}, Test MSE: {lstm_test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to combine the first part with stock selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ticker  predicted_return\n",
      "10049    MNY          0.101783\n",
      "184      WDC          0.095579\n",
      "28347    RCI          0.092300\n",
      "26530    RCI          0.087547\n",
      "4959     WDC          0.085358\n",
      "...      ...               ...\n",
      "45325   LLTC          0.061181\n",
      "33839    SFG          0.061044\n",
      "40986   SUNW          0.061025\n",
      "30350    CNO          0.060977\n",
      "8860      AW          0.060784\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "      ticker  predicted_return\n",
      "30647    AMD          0.074281\n",
      "41454   BRCD          0.072448\n",
      "27433     KG          0.072171\n",
      "29766    AMD          0.069326\n",
      "30604     MU          0.066990\n",
      "...      ...               ...\n",
      "11554   QLGC          0.050886\n",
      "35097    CCE          0.050817\n",
      "17861   ENDP          0.050806\n",
      "246     SEPR          0.050769\n",
      "39915   FDML          0.050765\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "      ticker  predicted_return\n",
      "27040    GNW          0.059678\n",
      "12075    CAR          0.051638\n",
      "23888    AMD          0.050113\n",
      "22470    GNW          0.048970\n",
      "13604   TMUS          0.047965\n",
      "...      ...               ...\n",
      "7828      MU          0.037013\n",
      "15302    GNW          0.036999\n",
      "38855    ORI          0.036990\n",
      "44658     MU          0.036937\n",
      "23366    CNO          0.036931\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "      ticker  predicted_return\n",
      "23820    OMF          0.054401\n",
      "8325     HPQ          0.049794\n",
      "28652    SPB          0.049657\n",
      "22901    OMF          0.048040\n",
      "25653    OMF          0.047901\n",
      "...      ...               ...\n",
      "27731    SPB          0.034413\n",
      "18937     ON          0.034396\n",
      "40662   ATUS          0.034370\n",
      "12478   ENTG          0.034348\n",
      "39616   ALLY          0.034272\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "      ticker  predicted_return\n",
      "32946   COOP          0.044559\n",
      "31870   COOP          0.044025\n",
      "3077    INTC          0.041521\n",
      "23351   SAFM          0.040997\n",
      "28548   COOP          0.040574\n",
      "...      ...               ...\n",
      "26046    APA          0.032632\n",
      "20277     AA          0.032626\n",
      "2996    GNTX          0.032592\n",
      "38235   AMKR          0.032556\n",
      "36092   COOP          0.032554\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to predict on next month and select top 100 stocks\n",
    "def predict_next_month(models, X_test, Y_mean, stock_ids, top_n=100):\n",
    "    predictions = {}\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model_name, (model, _) in models.items():\n",
    "        predictions[model_name] = model.predict(X_test) + Y_mean\n",
    "    \n",
    "    # OLS to begin\n",
    "    # Select OLS model for final selection to start - using other model selection methods - khai suggested looking into information coefficient \n",
    "    final_predictions = predictions['ols']\n",
    "    \n",
    "    # Create a DataFrame with predictions and stock ids\n",
    "    pred_df = pd.DataFrame({\n",
    "        'ticker': stock_ids,\n",
    "        'predicted_return': final_predictions\n",
    "    })\n",
    "    \n",
    "    # Sort by predicted return and select top N stocks\n",
    "    top_stocks = pred_df.sort_values(by='predicted_return', ascending=False).head(top_n)\n",
    "    \n",
    "    return top_stocks\n",
    "\n",
    "# Main execution\n",
    "#def main():\n",
    "# Prepare data\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train_val = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "        \n",
    "    # Further split train_val into train and validate\n",
    "    train_size = int(0.8 * len(train_val))\n",
    "    train = train_val[:train_size]\n",
    "    validate = train_val[train_size:]\n",
    "        \n",
    "    # Scale features\n",
    "    X_train, X_val, X_test = scale_features(train, validate, test, stock_vars)\n",
    "    Y_train, Y_val, Y_test = train[ret_var].values, validate[ret_var].values, test[ret_var].values\n",
    "        \n",
    "    # De-mean Y\n",
    "    Y_mean = np.mean(Y_train)\n",
    "    Y_train_dm = Y_train - Y_mean\n",
    "        \n",
    "    # Train models in parallel\n",
    "    model_results = parallel_model_training(X_train, Y_train_dm, X_val, Y_val, Y_mean)\n",
    "        \n",
    "    # Train LSTM \n",
    "    # lstm_model, lstm_val_mse = train_lstm(X_train, Y_train, X_val, Y_val)\n",
    "        \n",
    "    # Make predictions for the next month and select top 100 stocks\n",
    "    stock_ids = test['stock_ticker'].values  # using ticker for ID\n",
    "    top_100_stocks = predict_next_month(model_results, X_test, Y_mean, stock_ids, top_n=100)\n",
    "    \n",
    "    # Add top 100 stocks to portfolio (further logic can be implemented here)\n",
    "    print(top_100_stocks)\n",
    "\n",
    "# Additional steps:\n",
    "# 1. Calculate weights (cequal-weighted to start - can use predictions later)\n",
    "# 2. Add these stocks to portfolio and track performance\n",
    "# 3. Iterate the process and evaluate performance at each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding portfolio evaluaiton and better stock selection - still simple using nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ols - Validation MSE: 0.015196703399922797, Test MSE: 0.005724219654758669\n",
      "Portfolio Return (equal-weighted): 0.021320510831690034\n",
      "lasso - Validation MSE: 0.014119349956661777, Test MSE: 0.00533553671789525\n",
      "Portfolio Return (equal-weighted): 0.005594892693205861\n",
      "ridge - Validation MSE: 0.015196103962458237, Test MSE: 0.005724074469773026\n",
      "Portfolio Return (equal-weighted): 0.021320510831690034\n",
      "en - Validation MSE: 0.014119349956661777, Test MSE: 0.00533553671789525\n",
      "Portfolio Return (equal-weighted): 0.005594892693205861\n",
      "ols - Validation MSE: 0.005725876461242681, Test MSE: 0.01301053509817297\n",
      "Portfolio Return (equal-weighted): 0.00879326007626934\n",
      "lasso - Validation MSE: 0.00547020960108268, Test MSE: 0.0128663732618627\n",
      "Portfolio Return (equal-weighted): -0.001599930963518663\n",
      "ridge - Validation MSE: 0.005725836485825353, Test MSE: 0.01301048912892264\n",
      "Portfolio Return (equal-weighted): 0.00879326007626934\n",
      "en - Validation MSE: 0.00547020960108268, Test MSE: 0.0128663732618627\n",
      "Portfolio Return (equal-weighted): -0.001599930963518663\n",
      "ols - Validation MSE: 0.01009783362922321, Test MSE: 0.008457075284921666\n",
      "Portfolio Return (equal-weighted): 0.04149101277947498\n",
      "lasso - Validation MSE: 0.009911206346441886, Test MSE: 0.00822526988615247\n",
      "Portfolio Return (equal-weighted): 0.039548849963161506\n",
      "ridge - Validation MSE: 0.010097808296425901, Test MSE: 0.008457030758016041\n",
      "Portfolio Return (equal-weighted): 0.04149101277947498\n",
      "en - Validation MSE: 0.009911206346441886, Test MSE: 0.00822526988615247\n",
      "Portfolio Return (equal-weighted): 0.039548849963161506\n",
      "ols - Validation MSE: 0.0068048435258779546, Test MSE: 0.009657878496627172\n",
      "Portfolio Return (equal-weighted): -0.01519627950147815\n",
      "lasso - Validation MSE: 0.006689138649950775, Test MSE: 0.00937073663935976\n",
      "Portfolio Return (equal-weighted): -0.01345175554424297\n",
      "ridge - Validation MSE: 0.0068048321235268236, Test MSE: 0.009657862596704482\n",
      "Portfolio Return (equal-weighted): -0.01519627950147815\n",
      "en - Validation MSE: 0.006689138649950775, Test MSE: 0.00937073663935976\n",
      "Portfolio Return (equal-weighted): -0.01345175554424297\n",
      "ols - Validation MSE: 0.009565905753150366, Test MSE: 0.011868518704955202\n",
      "Portfolio Return (equal-weighted): 0.024917996502869622\n",
      "lasso - Validation MSE: 0.009449561973720103, Test MSE: 0.011920427622186015\n",
      "Portfolio Return (equal-weighted): 0.015161493034909572\n",
      "ridge - Validation MSE: 0.009565893606320847, Test MSE: 0.011868499330915439\n",
      "Portfolio Return (equal-weighted): 0.024917996502869622\n",
      "en - Validation MSE: 0.009449561973720103, Test MSE: 0.011920427622186015\n",
      "Portfolio Return (equal-weighted): 0.015161493034909572\n"
     ]
    }
   ],
   "source": [
    "# Function to select top 100 stocks based on predicted returns for the next month\n",
    "def select_top_stocks(predictions, stock_ids, top_n=100):\n",
    "    stock_returns_df = pd.DataFrame({\n",
    "        'stock_ticker': stock_ids,\n",
    "        'predicted_return': predictions\n",
    "    })\n",
    "    stock_returns_df = stock_returns_df.drop_duplicates(subset=['stock_ticker'])  # Ensure no duplicates\n",
    "    top_stocks = stock_returns_df.nlargest(top_n, 'predicted_return')\n",
    "    return top_stocks\n",
    "\n",
    "# Portfolio performance evaluation with equal weights\n",
    "def evaluate_portfolio(stock_data, selected_stocks, ret_var):\n",
    "    equal_weights = np.ones(len(selected_stocks)) / len(selected_stocks)  # Equal weight for each stock\n",
    "    selected_stock_data = stock_data[stock_data['permno'].isin(selected_stocks['stock_ticker'])]\n",
    "    \n",
    "    portfolio_return = (selected_stock_data.groupby('date')[ret_var].mean()).mean()  # Average return of the portfolio\n",
    "    print(f\"Portfolio Return (equal-weighted): {portfolio_return}\")\n",
    "    return portfolio_return\n",
    "\n",
    "# Main execution\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train_val = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "    \n",
    "    # Ensure that test set contains data for one full month\n",
    "    test = test[(test['date'].dt.month == (test['date'].dt.month.min() + 1))]  # Next month\n",
    "    \n",
    "    # Further split train_val into train and validate\n",
    "    train_size = int(0.8 * len(train_val))\n",
    "    train = train_val[:train_size]\n",
    "    validate = train_val[train_size:]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train, X_val, X_test = scale_features(train, validate, test, stock_vars)\n",
    "    Y_train, Y_val, Y_test = train[ret_var].values, validate[ret_var].values, test[ret_var].values\n",
    "    \n",
    "    # De-mean Y\n",
    "    Y_mean = np.mean(Y_train)\n",
    "    Y_train_dm = Y_train - Y_mean\n",
    "    \n",
    "    # Train models in parallel\n",
    "    model_results = parallel_model_training(X_train, Y_train_dm, X_val, Y_val, Y_mean)\n",
    "    \n",
    "    for model_name, (model, val_mse) in model_results.items():\n",
    "        test_pred = model.predict(X_test) + Y_mean\n",
    "        test_mse = mean_squared_error(Y_test, test_pred)\n",
    "        print(f\"{model_name} - Validation MSE: {val_mse}, Test MSE: {test_mse}\")\n",
    "        \n",
    "        # Select top 100 stocks for the next month\n",
    "        top_100_stocks2 = select_top_stocks(test_pred, test['permno'], top_n=100)\n",
    "        \n",
    "        # Evaluate the portfolio with equal weights\n",
    "        evaluate_portfolio(test, top_100_stocks2, ret_var)\n",
    "        \n",
    "    # LSTM model \n",
    "    # lstm_model, lstm_val_mse = train_lstm(X_train, Y_train, X_val, Y_val)\n",
    "    # with torch.no_grad():\n",
    "    #    lstm_test_pred = lstm_model(torch.FloatTensor(X_test)).squeeze().numpy()\n",
    "    #    lstm_test_mse = mean_squared_error(Y_test, lstm_test_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding next month stock feature by shifting values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akobe\\AppData\\Local\\Temp\\ipykernel_26940\\1540932337.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['next_month_stockExRet'] = data.groupby('stock_ticker')['stock_exret'].pct_change().shift(-1)\n"
     ]
    }
   ],
   "source": [
    "# some preprocessing\n",
    "data['next_month_stockExRet'] = data.groupby('stock_ticker')['stock_exret'].pct_change().shift(-1)\n",
    "data = data.drop(columns=['shrcd', 'ret_eom'])\n",
    "data = data[~np.isinf(data['next_month_stockExRet'])]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month                                     top_100_stocks  \\\n",
      "0   2023-01  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
      "1   2023-02  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
      "2   2023-03  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
      "3   2023-04  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
      "4   2023-05  [{'stock_ticker': 'COLM', 'predicted_return': ...   \n",
      "5   2023-06  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
      "6   2023-07  [{'stock_ticker': 'BBIO', 'predicted_return': ...   \n",
      "7   2023-08  [{'stock_ticker': 'IRTC', 'predicted_return': ...   \n",
      "8   2023-09  [{'stock_ticker': 'MARA', 'predicted_return': ...   \n",
      "9   2023-10  [{'stock_ticker': 'CERE', 'predicted_return': ...   \n",
      "10  2023-11  [{'stock_ticker': 'MARA', 'predicted_return': ...   \n",
      "11  2023-12  [{'stock_ticker': 'HMSY', 'predicted_return': ...   \n",
      "\n",
      "    average_actual_return  \n",
      "0               -2.358061  \n",
      "1               -1.273336  \n",
      "2               -0.053791  \n",
      "3                1.420487  \n",
      "4                0.482415  \n",
      "5               -4.366208  \n",
      "6               -2.051492  \n",
      "7               -1.108195  \n",
      "8              -10.847763  \n",
      "9               -0.759540  \n",
      "10              -1.653619  \n",
      "11            -630.138716  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge  # or any other model\n",
    "\n",
    "# Prepare the training data (up to 2009-12-31) - test OOS is from 2010 to 2023\n",
    "train_data = data[data['date'] <= '2009-12-31']  # All data until the end of 2009\n",
    "test_data = data[data['date'] >= '2010-01-01']   # Predict for 2010 and beyond\n",
    "\n",
    "# Drop unnecessary columns for training features and target\n",
    "X_train = train_data.drop(columns=['next_month_stockExRet', 'stock_ticker', 'stock_exret', 'date', 'size_port', 'cusip', 'comp_name'])\n",
    "y_train = train_data['next_month_stockExRet']\n",
    "\n",
    "# Train the model\n",
    "model = Ridge()  # random starting model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for the next 12 months - 2010\n",
    "results = []\n",
    "for month in range(1, 13):\n",
    "    # Extract the test data for the current month\n",
    "    X_test = test_data[test_data['date'].dt.month == month].drop(columns=['next_month_stockExRet', 'stock_ticker', 'stock_exret', 'date', 'size_port', 'cusip', 'comp_name'])\n",
    "    y_test = test_data[test_data['date'].dt.month == month]['next_month_stockExRet']\n",
    "    \n",
    "    # Ensure there's test data for this month\n",
    "    if X_test.empty:\n",
    "        print(f\"No test data available for month {month}\")\n",
    "        continue\n",
    "    \n",
    "    # Predict the next month's returns\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get the stock tickers for the test set\n",
    "    stock_tickers = test_data[test_data['date'].dt.month == month]['stock_ticker'].values\n",
    "    \n",
    "    # Create a DataFrame with stock tickers and predicted returns\n",
    "    stock_predictions = pd.DataFrame({'stock_ticker': stock_tickers, 'predicted_return': y_pred})\n",
    "    \n",
    "    # Rank stocks based on predicted returns and select the top 100\n",
    "    top_100_stocks = stock_predictions.nlargest(100, 'predicted_return')\n",
    "    \n",
    "    # Calculate the average actual return of the top 100 stocks\n",
    "    actual_returns = test_data[\n",
    "        (test_data['stock_ticker'].isin(top_100_stocks['stock_ticker'])) & \n",
    "        (test_data['date'].dt.month == month)\n",
    "    ]['next_month_stockExRet'].mean()\n",
    "    \n",
    "    # Store the results for this month, including both stock tickers and predicted returns\n",
    "    results.append({\n",
    "        'month': f'2023-{month:02d}',\n",
    "        'top_100_stocks': top_100_stocks[['stock_ticker', 'predicted_return']].to_dict('records'),  # List of top stocks and their predicted returns\n",
    "        'average_actual_return': actual_returns\n",
    "    })\n",
    "    \n",
    "# Create a DataFrame to summarize the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the final results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "something seems off with 12th month predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>top_100_stocks</th>\n",
       "      <th>average_actual_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>[{'stock_ticker': 'FOSL', 'predicted_return': ...</td>\n",
       "      <td>-2.358061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>[{'stock_ticker': 'FOSL', 'predicted_return': ...</td>\n",
       "      <td>-1.273336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>[{'stock_ticker': 'FOSL', 'predicted_return': ...</td>\n",
       "      <td>-0.053791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04</td>\n",
       "      <td>[{'stock_ticker': 'FOSL', 'predicted_return': ...</td>\n",
       "      <td>1.420487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>[{'stock_ticker': 'COLM', 'predicted_return': ...</td>\n",
       "      <td>0.482415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06</td>\n",
       "      <td>[{'stock_ticker': 'FOSL', 'predicted_return': ...</td>\n",
       "      <td>-4.366208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07</td>\n",
       "      <td>[{'stock_ticker': 'BBIO', 'predicted_return': ...</td>\n",
       "      <td>-2.051492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>[{'stock_ticker': 'IRTC', 'predicted_return': ...</td>\n",
       "      <td>-1.108195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09</td>\n",
       "      <td>[{'stock_ticker': 'MARA', 'predicted_return': ...</td>\n",
       "      <td>-10.847763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>[{'stock_ticker': 'CERE', 'predicted_return': ...</td>\n",
       "      <td>-0.759540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11</td>\n",
       "      <td>[{'stock_ticker': 'MARA', 'predicted_return': ...</td>\n",
       "      <td>-1.653619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12</td>\n",
       "      <td>[{'stock_ticker': 'HMSY', 'predicted_return': ...</td>\n",
       "      <td>-630.138716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month                                     top_100_stocks  \\\n",
       "0   2023-01  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
       "1   2023-02  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
       "2   2023-03  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
       "3   2023-04  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
       "4   2023-05  [{'stock_ticker': 'COLM', 'predicted_return': ...   \n",
       "5   2023-06  [{'stock_ticker': 'FOSL', 'predicted_return': ...   \n",
       "6   2023-07  [{'stock_ticker': 'BBIO', 'predicted_return': ...   \n",
       "7   2023-08  [{'stock_ticker': 'IRTC', 'predicted_return': ...   \n",
       "8   2023-09  [{'stock_ticker': 'MARA', 'predicted_return': ...   \n",
       "9   2023-10  [{'stock_ticker': 'CERE', 'predicted_return': ...   \n",
       "10  2023-11  [{'stock_ticker': 'MARA', 'predicted_return': ...   \n",
       "11  2023-12  [{'stock_ticker': 'HMSY', 'predicted_return': ...   \n",
       "\n",
       "    average_actual_return  \n",
       "0               -2.358061  \n",
       "1               -1.273336  \n",
       "2               -0.053791  \n",
       "3                1.420487  \n",
       "4                0.482415  \n",
       "5               -4.366208  \n",
       "6               -2.051492  \n",
       "7               -1.108195  \n",
       "8              -10.847763  \n",
       "9               -0.759540  \n",
       "10              -1.653619  \n",
       "11            -630.138716  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note this is initial tests and possible that this approach is not correct. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiamVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
